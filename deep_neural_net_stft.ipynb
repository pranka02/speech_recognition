{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf \n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "      <th>1025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071075</td>\n",
       "      <td>0.103780</td>\n",
       "      <td>0.132942</td>\n",
       "      <td>0.145809</td>\n",
       "      <td>0.155422</td>\n",
       "      <td>0.171643</td>\n",
       "      <td>0.197313</td>\n",
       "      <td>0.213961</td>\n",
       "      <td>0.281038</td>\n",
       "      <td>0.397688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.982549</td>\n",
       "      <td>1.533908</td>\n",
       "      <td>0.231439</td>\n",
       "      <td>0.103404</td>\n",
       "      <td>0.095910</td>\n",
       "      <td>0.216689</td>\n",
       "      <td>0.235391</td>\n",
       "      <td>0.081970</td>\n",
       "      <td>0.056954</td>\n",
       "      <td>0.051746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.971369</td>\n",
       "      <td>1.522197</td>\n",
       "      <td>0.194524</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>0.096635</td>\n",
       "      <td>0.218484</td>\n",
       "      <td>0.246341</td>\n",
       "      <td>0.082754</td>\n",
       "      <td>0.056002</td>\n",
       "      <td>0.051888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016832</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>0.070817</td>\n",
       "      <td>0.088907</td>\n",
       "      <td>0.097617</td>\n",
       "      <td>0.098459</td>\n",
       "      <td>0.094711</td>\n",
       "      <td>0.091990</td>\n",
       "      <td>0.181514</td>\n",
       "      <td>0.195997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022590</td>\n",
       "      <td>0.053877</td>\n",
       "      <td>0.085716</td>\n",
       "      <td>0.106991</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>0.136774</td>\n",
       "      <td>0.132022</td>\n",
       "      <td>0.156490</td>\n",
       "      <td>0.155312</td>\n",
       "      <td>0.123398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.591010</td>\n",
       "      <td>3.844878</td>\n",
       "      <td>0.595446</td>\n",
       "      <td>0.381324</td>\n",
       "      <td>0.246885</td>\n",
       "      <td>0.392816</td>\n",
       "      <td>0.401681</td>\n",
       "      <td>0.206578</td>\n",
       "      <td>0.304335</td>\n",
       "      <td>0.507444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.071075  0.103780  0.132942  0.145809  0.155422  0.171643  0.197313   \n",
       "1  2.982549  1.533908  0.231439  0.103404  0.095910  0.216689  0.235391   \n",
       "2  2.971369  1.522197  0.194524  0.121648  0.096635  0.218484  0.246341   \n",
       "3  0.016832  0.039279  0.070817  0.088907  0.097617  0.098459  0.094711   \n",
       "4  0.022590  0.053877  0.085716  0.106991  0.110541  0.136774  0.132022   \n",
       "5  7.591010  3.844878  0.595446  0.381324  0.246885  0.392816  0.401681   \n",
       "\n",
       "       7         8         9     ...       1016      1017      1018      1019  \\\n",
       "0  0.213961  0.281038  0.397688  ...   0.000046  0.000051  0.000056  0.000061   \n",
       "1  0.081970  0.056954  0.051746  ...   0.000107  0.000119  0.000128  0.000135   \n",
       "2  0.082754  0.056002  0.051888  ...   0.000202  0.000214  0.000227  0.000240   \n",
       "3  0.091990  0.181514  0.195997  ...   0.000005  0.000004  0.000006  0.000007   \n",
       "4  0.156490  0.155312  0.123398  ...   0.000023  0.000021  0.000019  0.000016   \n",
       "5  0.206578  0.304335  0.507444  ...   0.000173  0.000148  0.000120  0.000094   \n",
       "\n",
       "       1020      1021      1022      1023      1024  1025  \n",
       "0  0.000064  0.000067  0.000070  0.000071  0.000072   bed  \n",
       "1  0.000140  0.000143  0.000144  0.000145  0.000146   bed  \n",
       "2  0.000253  0.000265  0.000273  0.000279  0.000281   bed  \n",
       "3  0.000008  0.000009  0.000009  0.000010  0.000010   bed  \n",
       "4  0.000013  0.000010  0.000008  0.000007  0.000007   bed  \n",
       "5  0.000099  0.000121  0.000139  0.000151  0.000155   bed  \n",
       "\n",
       "[6 rows x 1026 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_full/stft.csv',header = None)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "      <th>1025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26744</th>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.022073</td>\n",
       "      <td>0.035980</td>\n",
       "      <td>0.036198</td>\n",
       "      <td>0.050608</td>\n",
       "      <td>0.056913</td>\n",
       "      <td>0.057419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19919</th>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>0.013523</td>\n",
       "      <td>0.017098</td>\n",
       "      <td>0.042027</td>\n",
       "      <td>0.069646</td>\n",
       "      <td>0.149887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9509</th>\n",
       "      <td>0.843857</td>\n",
       "      <td>1.572007</td>\n",
       "      <td>2.507611</td>\n",
       "      <td>2.084646</td>\n",
       "      <td>1.627376</td>\n",
       "      <td>2.558883</td>\n",
       "      <td>2.319095</td>\n",
       "      <td>0.758498</td>\n",
       "      <td>0.556252</td>\n",
       "      <td>0.412845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15078</th>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.068054</td>\n",
       "      <td>0.069575</td>\n",
       "      <td>0.096505</td>\n",
       "      <td>0.138729</td>\n",
       "      <td>0.256232</td>\n",
       "      <td>0.426176</td>\n",
       "      <td>0.519889</td>\n",
       "      <td>0.650789</td>\n",
       "      <td>0.768283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50204</th>\n",
       "      <td>0.656810</td>\n",
       "      <td>1.132745</td>\n",
       "      <td>1.473270</td>\n",
       "      <td>1.567249</td>\n",
       "      <td>1.356326</td>\n",
       "      <td>1.810747</td>\n",
       "      <td>2.533035</td>\n",
       "      <td>2.554930</td>\n",
       "      <td>2.340737</td>\n",
       "      <td>2.198883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32819</th>\n",
       "      <td>0.071105</td>\n",
       "      <td>0.084313</td>\n",
       "      <td>0.080176</td>\n",
       "      <td>0.060777</td>\n",
       "      <td>0.059927</td>\n",
       "      <td>0.073869</td>\n",
       "      <td>0.086010</td>\n",
       "      <td>0.094393</td>\n",
       "      <td>0.139742</td>\n",
       "      <td>0.404511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "26744  0.011449  0.008195  0.006956  0.011483  0.022073  0.035980  0.036198   \n",
       "19919  0.004943  0.005227  0.005504  0.006346  0.011776  0.013523  0.017098   \n",
       "9509   0.843857  1.572007  2.507611  2.084646  1.627376  2.558883  2.319095   \n",
       "15078  0.066055  0.068054  0.069575  0.096505  0.138729  0.256232  0.426176   \n",
       "50204  0.656810  1.132745  1.473270  1.567249  1.356326  1.810747  2.533035   \n",
       "32819  0.071105  0.084313  0.080176  0.060777  0.059927  0.073869  0.086010   \n",
       "\n",
       "           7         8         9     ...        1016      1017      1018  \\\n",
       "26744  0.050608  0.056913  0.057419  ...    0.000008  0.000004  0.000008   \n",
       "19919  0.042027  0.069646  0.149887  ...    0.000027  0.000024  0.000022   \n",
       "9509   0.758498  0.556252  0.412845  ...    0.000154  0.000104  0.000061   \n",
       "15078  0.519889  0.650789  0.768283  ...    0.000063  0.000069  0.000075   \n",
       "50204  2.554930  2.340737  2.198883  ...    0.000834  0.001547  0.002287   \n",
       "32819  0.094393  0.139742  0.404511  ...    0.000004  0.000002  0.000003   \n",
       "\n",
       "           1019      1020      1021      1022      1023      1024   1025  \n",
       "26744  0.000013  0.000018  0.000022  0.000025  0.000027  0.000027   nine  \n",
       "19919  0.000021  0.000020  0.000019  0.000018  0.000017  0.000017  happy  \n",
       "9509   0.000110  0.000163  0.000211  0.000248  0.000272  0.000280  eight  \n",
       "15078  0.000081  0.000085  0.000089  0.000092  0.000094  0.000095   four  \n",
       "50204  0.002985  0.003606  0.004119  0.004502  0.004739  0.004819  three  \n",
       "32819  0.000005  0.000007  0.000010  0.000011  0.000012  0.000012    off  \n",
       "\n",
       "[6 rows x 1026 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 60000\n",
    "features = np.array(df)\n",
    "tr_x = features[:tr,0:1024]\n",
    "tr_y = features[:tr,1025]\n",
    "val_x = features[tr:,0:1024]\n",
    "val_y = features[tr:,1025]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mean = np.mean(tr_x, axis=0)\n",
    "tr_std = np.std(tr_x, axis = 0,dtype=np.float64)\n",
    "tr_xp = (tr_x - tr_mean[None,:])/tr_std[None,:]\n",
    "\n",
    "val_mean = np.mean(val_x, axis=0)\n",
    "val_std = np.std(val_x, axis = 0,dtype=np.float64)\n",
    "val_xp = (val_x - val_mean[None,:])/val_std[None,:]\n",
    "\n",
    "lab = LabelEncoder()\n",
    "tr_yp= lab.fit_transform(tr_y)\n",
    "val_yp= lab.fit_transform(val_y)\n",
    "\n",
    "n_words= np.unique(tr_yp)\n",
    "n_classes = len(n_words)\n",
    "n_features = tr_xp.shape[1]\n",
    "\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(n_classes))\n",
    "tr_yoh = label_binarizer.transform(tr_yp)\n",
    "val_yoh = label_binarizer.transform(val_yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components ='mle', svd_solver = 'full')\n",
    "pca.fit(tr_xp)\n",
    "tr_pca = pca.transform(tr_xp)\n",
    "val_pca = pca.transform(val_xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = decomposition.PCA(n_components =120)\n",
    "pca1.fit(tr_xp)\n",
    "tr_pca1 = pca1.transform(tr_xp)\n",
    "val_pca1 = pca1.transform(val_xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "x = tf.placeholder('float',[None,120])\n",
    "y = tf.placeholder('float',[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "\n",
    "    # initializing weights and biases for the hidden layers and the output layer\n",
    "    hidden_l1 = {'weights': tf.Variable(tf.random_normal([120,n_nodes_hl1])), \n",
    "                 'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_l2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])), \n",
    "                 'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_l3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])), \n",
    "                 'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_l = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])), \n",
    "                 'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    # computing the ouput of each layer and applying a non-linearity at the end of each layer computation\n",
    "    l1 = tf.add(tf.matmul(data, hidden_l1['weights']), hidden_l1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_l2['weights']), hidden_l2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_l3['weights']), hidden_l3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.add(tf.matmul(l3, output_l['weights']), output_l['biases'])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "\n",
    "    # learning rate = 0.001\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # one epoch = one cycle of feed-forward and backprop\n",
    "    n_epochs = 200 \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            for index, offset in enumerate(range(0, tr, batch_size)):\n",
    "                x_epoch, y_epoch = np.array(tr_pca1[offset: offset + batch_size,:]), np.array(tr_yoh[offset: offset + batch_size])\n",
    "                _, c = sess.run([optimizer, cost], feed_dict ={x:x_epoch, y:y_epoch}) \n",
    "                epoch_loss +=c\n",
    "            if (epoch%50 == 0):\n",
    "                print ('Epoch', epoch, 'completed out of', n_epochs, 'loss:', epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        accuracy  = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x:val_pca1, y:val_yoh}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 200 loss: 10308673.4209\n",
      "Epoch 50 completed out of 200 loss: 62116.3194427\n",
      "Epoch 100 completed out of 200 loss: 23390.2148476\n",
      "Epoch 150 completed out of 200 loss: 13188.6393452\n",
      "Accuracy: 0.338864\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
